#python #генератор #конвееры_генераторов


У итераторов есть замечательная особенность: их можно комбинировать. Это позволяет вместо огромных циклов с перемешанными этапами обработки писать небольшие блоки, которые стыкуются друг с другом.

Генераторы являются итераторами, поэтому мы можем компоновать их, создавая тем самым так называемые конвейеры генераторов
```python
def integers(n):
    for i in range(1, n + 1):
        yield i

def evens(iterable):
    for i in iterable:
        if not i % 2:
            yield i

def squared(iterable):
    for i in iterable:
        yield i * i

def negated(iterable):
    for i in iterable:
        yield -i

chain = negated(squared(evens(integers(10))))

print(*chain)
```
```
-4 -16 -36 -64 -100
```
Самое главное в конвейерах генераторов это то, что обработка данных происходит по одному элементу за раз. Между этапами обработки в цепочке нет буферизации: генератор целых чисел выдает единственное значение, сажем, 4. Это активирует генератор четных чисел, который обрабатывает это значение, и поскольку оно четное, передает его на следующий этап в виде числа `4`. Далее активируется генератор квадратов, который обрабатывает значение `4` и передает его на следующий этап как `4**2 = 16`. Далее активируется генератор инвертирования знака, который изменяет знак числа `16` на противоположный и порождает значение - `16`

Мы можем продолжать расширять эту цепочку генераторов, чтобы построить конвейер обработки со многими шагами. Он по-прежнему будет продолжать работать эффективно и может быть легко изменен, поскольку каждый шаг в цепочке представляет собой отдельную генераторную функцию

Конвейеры данных выглядят еще нагляднее, если они построены на основе генераторных выражений
```python
n = 10

integers = (i for i in range(1, n + 1))
evens = (i for i in integers if not i % 2)
squared = (i * i for i in evens)
negated = (-i for i in squared)

print(*negated)
```
Этот код полностью аналогичен примеру выше.

#### Обработка больших файлов с помощью конвейерных генераторов
Конвейеры данных, построенные на генераторах позволяют скомпоновать код для обработки больших наборов данных без использования большого количества памяти. Представим, что у нас есть очень большой текстовый файл в формате `csv` с именем `data.csv`, содержащий данные посещения сайтов, которые нужно обработать

Заголовок файла `data.csv` имеет вид:
```
user_id,user_name,user_ip,date,page_url
```
Для обработки такого файла необходимо проделать следующие шаги:
- открыть файл для чтения
- прочитать каждую строку файла
- разбить каждую строку через разделитель `,` на список значений
- извлечь имена столбцов
- использовать имена столбцов и список значений из строк для создания словарей
- отфильтровать не подходящие строки
- обработать интересующие значения
Все указанные выше шаги можно выполнить при помощи нескольких генераторов

**Шаг 1** Открываем файл `data.csv` для чтения:
```python
with open('data.csv', 'r', encoding='utf-8') as file: 
	# обрабатываем содержимое файла
```
**Шаг 2.** Читаем каждую строку с помощью генераторного выражения:
```python
with open('data.csv', 'r', encoding='utf-8') as file:
    file_lines = (line for line in file)
```
**Шаг 3.** Разбиваем строки файла через  разделитель `,` на список значений:
```python
with open('data.csv', 'r', encoding='utf-8') as file:
    file_lines = (line for line in file)
    line_values = (line.rstrip().split(',') for line in file_lines)
```
**Шаг 4.** Извлекаем первую строку, которая является названием столбцов:
```python
with open('data.csv', 'r', encoding='utf-8') as file:
    file_lines = (line for line in file)
    line_values = (line.rstrip().split(',') for line in file_lines)
    file_headers = next(line_values)
```
**Шаг 5.** Соединяем названия столбцов с соответствующими им значениями:
```python
with open('data.csv', 'r', encoding='utf-8') as file:
    file_lines = (line for line in file)
    line_values = (line.rstrip().split(',') for line in file_lines)
    file_headers = next(line_values)
    line_dicts = (dict(zip(file_headers, data)) for data in line_values)
```
**Шаг 6.** Фильтруем неподходящие строки и оставляем только те, в которых столбец `page_url` содержит текст `stepik.org`:
```python
with open('data.csv', 'r', encoding='utf-8') as file:
    file_lines = (line for line in file)
    line_values = (line.rstrip().split(',') for line in file_lines)
    file_headers = next(line_values)
    line_dicts = (dict(zip(file_headers, data)) for data in line_values)

    result = (
        (line['user_name'], line['user_ip'])
        for line in line_dicts
        if 'stepik.org' in line['page_url']
        )
    
```
Необходимо понимать и помнить, что приведенный выше код не перебирает содержимое в генераторе `result`. Ничего не будет исполняться, пока не будет задействована итерация (явная или неявная) по генератору `result`.

**Шаг 7.** Обрабатываем интересующие значения:
```python
with open('data.csv', 'r', encoding='utf-8') as file:
    file_lines = (line for line in file)
    line_values = (line.rstrip().split(',') for line in file_lines)
    file_headers = next(line_values)
    line_dicts = (dict(zip(file_headers, data)) for data in line_values)

    result = (
        (line['user_name'], line['user_ip'])
        for line in line_dicts
        if 'stepik.org' in line['page_url']
        )

    for index, (name, ip) in enumerate(set(result), 1):
        print(f'{index}. {name} --- {ip}')
    
```
`set()` заставляет работать все созданные генераторы вместе. Все они функционируют как один конвейер больших данных, при этом в память попадают только нужные строки

#### Производительность при использовании генераторов
Как уже было сказано, генераторы идеально  подходят, когда необходимо прочитать содержимое большого файла

Чтобы наглядно показать преимущество использования генераторов, создадим две функции `read_immediatly()` и `read_lazy()`, которые читают содержимое текстового файла:
- первая функция читает все непустые строки файла и заносит их в список, который затем возвращает с помощью оператора `return`
- вторая функция читает все непустые строки файла и возвращает их с помощью оператора `yield`
```python
def read_immediately(file_name):
    with open(file_name, 'r', encoding='utf-8') as file:
        result = []
        for line in file:
            line = line.rstrip('\n')
            if line != '':
                result.append(line)
        return result

def read_lazy(file_name):
    with open(file_name, 'r', encoding='utf-8') as file:
        for line in file:
            line = line.rstrip('\n')
            if line != '':
                yield line
```
Запустив указанные функции на файлах разных размеров, получим следующие результаты:

| Размер файла | **return** |          | **yield**  |          |
| :----------: | :--------: | :------: | :--------: | :------: |
|              |   Память   |  Время   |   Память   |  Время   |
|   4 Кбайт    | 5,3 Мбайт  | 0.023 с  | 5,42 Мбайт |  0.08 c  |
|  324 Кбайт   | 9,98 Мбайт | 0.028 с  | 5,37 Мбайт |  0,32 с  |
|   26 Мбайт   | 392 Мбайт  |   27 с   | 5.52 Мбайт | 29.61 с  |
|  263 Мбайт   | 3,65 Гбайт | 273.56 с | 5,55 Мбайт | 292,99 с |

Видно, что в обоих случаях время увеличивается с примерно одинаковой скоростью, а количество потребляемой памяти сильно различается. Чем больше обрабатываемый файл, тем заметнее различие.