#Алгоритмы

![[Pasted image 20250425180359.png]]
### Асимптотический анализ
Когда мы говорим об изменении сложности алгоритмов, мы подразумеваем анализ времени, которое потребуется для обработки очень большого набора данных. Такой анализ называют асимптотическими. Сколько времени потребуется на обработку массива из десяти элементов? Тысячи? Десяти миллионов? Если алгоритм обрабатывает тысячу элементов за пять миллисекунд, что случится, если мы передадим в него миллион? Будет ли он выполняться пять минут или пять лет?
#### Порядок роста
Порядок роста описывает то, как сложность алгоритма растет с увеличением размера датасета. Чаще всего он представлен в виде **O**-нотации: **O(f(x))**, где **f(x)** - формула, выражающая сложность алгоритма. В формуле может присутствовать переменная **n**, представляющая размер входных данных. Ниже приводится список наиболее часто встречающихся порядков роста, но он ни в коем случае не полный.
#### Константный - О(1)
Порядок роста **O(1)** означает, что вычислительная сложность алгоритма не зависит от размера выходных данных. Следует помнить, однако, что единица в формуле не значит, что алгоритм выполняется за одну операцию или требует очень мало времени. Он может потребовать и микросекунду, и год. Важно то, что это время не зависит от входных данных. 
![[Pasted image 20250429160453.png]]
**X** - размер датасета | **Y** - алоритмическая сложность
```python
def GetCount(item: list) -> int:
	return len(item)
```
#### Линейный - О(n)
Порядок **O(n)** означает, что сложность алгоритма линейно растет с увеличением входного массива. Если линейный алгоритм обрабатывает один элемент пять миллисекунд, то мы можем ожидать, что тысячу элементов он обрабатывает за пять секунд.

Такие алгоритмы легко узнать по наличию цикла по каждому элементу входного массива.
![[Pasted image 20250429160908.png]]
**X** - размер датасета | **Y** - алоритмическая сложность
```python
def GetSum(item: list[int]) -> int:
	sum = 0
	for i in item:
		sum += i
	return sum
```
#### Логарифмический - O(log n)
Порядок роста **O(log n)** означает, что время выполнения алгоритма растет логарифмически с увеличением размера входного массива. Большинство алгоритмов, работающих по принципу "деления  пополам", имеют логарифмическую сложность. Метод **Contains** бинарного дерева поиска (*binary search tree*) также имеет порядок роста **O(log n)**.
![[Pasted image 20250429161807.png]]
**X** - размер датасета | **Y** - алоритмическая сложность
Напишем функцию. которая находит объект `target` в списке `arr`
```python
def binary_search(arr: list, target: int) -> int:
    left, right = 0, len(arr) - 1
    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1
    return -1
```
Количество операций, которые выполнит эта функция зависит от того, как укомплектован датасет и как рано очередной обьект в нем окажется эквивалентен объекту `target`.
#### Линеарифметический - O(n * log n)
Линеарифметический (или линейно-логарифмический) алгоритм имеет порядок роста **O(n log n)**. Некоторые алгоритмы типа *разделяй и властвуй* попадают в эту категорию. В следующих частях мы увидим два таких примера - сортировка слиянием и быстрая сортировка.
![[Pasted image 20250429163053.png]]
**X** - размер датасета | **Y** - алоритмическая сложность
Классическим примером такой сложности является алгоритм сортировки слиянием.
```python
def merge_sort(arr: list):
    if len(arr) <= 1:
        return arr

    # Разделяем массив на две части
    mid = len(arr) // 2
    left_half = merge_sort(arr[:mid])
    right_half = merge_sort(arr[mid:])

    # Сливаем отсортированные половинки
    return merge(left_half, right_half)

def merge(left: list, right: list):
    merged = []
    left_index, right_index = 0, 0

    while left_index < len(left) and right_index < len(right):
        if left[left_index] < right[right_index]:
            merged.append(left[left_index])
            left_index += 1
        else:
            merged.append(right[right_index])
            right_index += 1

    # Добавляем оставшиеся элементы
    merged.extend(left[left_index:])
    merged.extend(right[right_index:])
    return merged

merge_sort(arr)
```
Че вообще за сортировка слиянием:
Возьмём массив `[6, 3, 8, 5, 2, 7, 4, 1]`:
1. **Разделение**:
    - `[6, 3, 8, 5]` и `[2, 7, 4, 1]`
    - `[6, 3]`, `[8, 5]`, `[2, 7]`, `[4, 1]`
    - `[6]`, `[3]`, `[8]`, `[5]`, `[2]`, `[7]`, `[4]`, `[1]`
2. **Слияние**:
    - `[6]` и `[3]` → `[3, 6]`
    - `[8]` и `[5]` → `[5, 8]`
    - `[2]` и `[7]` → `[2, 7]`
    - `[4]` и `[1]` → `[1, 4]`
3. **Объединение групп**:
    - `[3, 6]` и `[5, 8]` → `[3, 5, 6, 8]`
    - `[2, 7]` и `[1, 4]` → `[1, 2, 4, 7]
4. **Последнее слияние**:
    - `[3, 5, 6, 8]` и `[1, 2, 4, 7]` → `[1, 2, 3, 4, 5, 6, 7, 8]`
#### Квадратичный - O(n^2)
Время работы алгоритма с порядком роста **O(n^2)** зависит от квадрата размера входного массива. Несмотря на то, что такой ситуации иногда не избежать, квадратичная сложность - повод пересмотреть используемые алгоритмы или структуры данных. Проблема в том, что они плохо масштабируются. Например, если массив из тысячи элементов потребует `1 000 000` операций, массив из миллиона элементов потребует `1 000 000 000 000` операций. Если одна операция требует миллион элементов `32` года. Даже если он будет в сто раз быстрее, работа займет `84` дня.
![[Pasted image 20250429164917.png]]
**X** - размер датасета | **Y** - алоритмическая сложность
```python
def MegaSum(item: list):
	sum = 0
	for i in item:
		for j in item:
			sum += i + j
	return sum
```
